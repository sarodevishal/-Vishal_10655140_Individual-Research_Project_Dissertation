{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarodevishal/-Vishal_10655140_Individual-Research_Project_Dissertation/blob/master/CYyber_bullying_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw5TN1zDPaXQ"
      },
      "outputs": [],
      "source": [
        "pip install tflearn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRM2rLu1T-O4",
        "outputId": "8022b311-6905-4666-9ab9-bab25b5564b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_ZqfbN7PuOZ"
      },
      "outputs": [],
      "source": [
        " from keras.layers import merge "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EchkU9PtQt-j"
      },
      "outputs": [],
      "source": [
        "import tensorflow.python.keras.engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMABDNoqRVYJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow.python.keras.engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_iFFdheRVdn",
        "outputId": "cc9efb84-7bf7-450d-a064-93021e89f81a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement keras.engine.topology (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for keras.engine.topology\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install keras.engine.topology"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tflearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELtQ8pj4kSQj",
        "outputId": "cc16aa86-03a3-4021-fc1a-9e0a9c48dd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5DPW4yTRVkr"
      },
      "outputs": [],
      "source": [
        "pip install 'keras==2.1.6'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIImSuFYRVnw"
      },
      "outputs": [],
      "source": [
        "#Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtjZifCSPuQ_"
      },
      "outputs": [],
      "source": [
        "import tflearn\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from tflearn.layers.core import input_data, dropout, fully_connected\n",
        "from tflearn.layers.conv import conv_1d, global_max_pool\n",
        "from tflearn.layers.merge_ops import merge\n",
        "from tflearn.layers.estimator import regression\n",
        "import tensorflow as tf\n",
        "import os\n",
        "os.environ['KERAS_BACKEND']='theano'\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, merge, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras.models import Model,Sequential\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras import initializers, optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D6bi4u7RFev"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bjRZ36b1YHox",
        "outputId": "9744a063-b1a3-4e65-9ace-685571b2647f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.4.1\n",
            "  Downloading tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.17.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.6.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.0)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 70.8 MB/s \n",
            "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.8.0)\n",
            "Collecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 60.5 MB/s \n",
            "\u001b[?25hCollecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.37.1)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 72.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.2)\n",
            "Collecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.2.0)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68723 sha256=bbf71475cf2ab0553e30b419cc579109902e0205eaebc4e3441d3e60f1fb2388\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, numpy, grpcio, absl-py, wrapt, tensorflow-estimator, h5py, gast, flatbuffers, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.47.0\n",
            "    Uninstalling grpcio-1.47.0:\n",
            "      Successfully uninstalled grpcio-1.47.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.2.0\n",
            "    Uninstalling absl-py-1.2.0:\n",
            "      Successfully uninstalled absl-py-1.2.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.15.0 flatbuffers-1.12 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 numpy-1.19.5 tensorflow-2.4.1 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "absl",
                  "flatbuffers",
                  "gast",
                  "h5py",
                  "numpy",
                  "tensorflow",
                  "typing_extensions",
                  "wrapt"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install tensorflow==2.4.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "WgaIX8CQYRAz",
        "outputId": "cb3a3ece-79aa-4176-b4f5-e3ba38efe801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.4.3\n",
            "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (1.7.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (1.21.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.4.3) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.4.3) (1.5.2)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: Keras 2.1.6\n",
            "    Uninstalling Keras-2.1.6:\n",
            "      Successfully uninstalled Keras-2.1.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires keras<2.9,>=2.8.0rc0, but you have keras 2.4.3 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-2.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install keras==2.4.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding\n",
        "from keras.layers import Dense, Input, Flatten\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, merge, Dropout, LSTM, GRU, Bidirectional\n",
        "from keras.models import Model,Sequential\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "from keras import initializers, optimizers"
      ],
      "metadata": {
        "id": "zX-UTX47SfpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEoXK9KJPuTW"
      },
      "outputs": [],
      "source": [
        "def lstm_keras(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
        "#     K.clear_session()\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim, trainable=True))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(LSTM(embed_size))\n",
        "    model.add(Dropout(0.50))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    print ('model.summary()')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Pqb6DKYPuVa"
      },
      "outputs": [],
      "source": [
        "def cnn(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
        "    tf.reset_default_graph()\n",
        "    network = input_data(shape=[None, inp_dim], name='input')\n",
        "    network = tflearn.embedding(network, input_dim=vocab_size, output_dim=embed_size, name=\"EmbeddingLayer\")\n",
        "    network = dropout(network, 0.25)\n",
        "    branch1 = conv_1d(network, embed_size, 3, padding='valid', activation='relu', regularizer=\"L2\", name=\"layer_1\")\n",
        "    branch2 = conv_1d(network, embed_size, 4, padding='valid', activation='relu', regularizer=\"L2\", name=\"layer_2\")\n",
        "    branch3 = conv_1d(network, embed_size, 5, padding='valid', activation='relu', regularizer=\"L2\", name=\"layer_3\")\n",
        "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
        "    network = tf.expand_dims(network, 2)\n",
        "    network = global_max_pool(network)\n",
        "    network = dropout(network, 0.50)\n",
        "    network = fully_connected(network, num_classes, activation='softmax', name=\"fc\")\n",
        "    network = regression(network, optimizer='adam', learning_rate=learn_rate,\n",
        "                         loss='categorical_crossentropy', name='target')\n",
        "    \n",
        "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkMySICEPuXw"
      },
      "outputs": [],
      "source": [
        "def blstm(inp_dim,vocab_size, embed_size, num_classes, learn_rate):   \n",
        "#     K.clear_session()\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim, trainable=True))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Bidirectional(LSTM(embed_size)))\n",
        "    model.add(Dropout(0.50))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic4pKdYubESb"
      },
      "outputs": [],
      "source": [
        "class AttLayer(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        self.W = self.add_weight(name='kernel', \n",
        "                                      shape=(input_shape[-1],),\n",
        "                                      initializer='random_normal',\n",
        "                                      trainable=True)\n",
        "        super(AttLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
        "    def call(self, x, mask=None):\n",
        "        eij = K.tanh(K.dot(x, self.W))\n",
        "        \n",
        "        ai = K.exp(eij)\n",
        "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
        "        \n",
        "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
        "        return weighted_input.sum(axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37XWTIQmbEWC"
      },
      "outputs": [],
      "source": [
        "def blstm_atten(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
        "#     K.clear_session()\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Bidirectional(LSTM(embed_size, return_sequences=True)))\n",
        "    model.add(AttLayer())\n",
        "    model.add(Dropout(0.50))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    adam = optimizers.Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GJhEjMMbEZr"
      },
      "outputs": [],
      "source": [
        "def get_model(m_type,inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
        "    if m_type == 'cnn':\n",
        "        model = cnn(inp_dim, vocab_size, embed_size, num_classes, learn_rate)\n",
        "    elif m_type == 'lstm':\n",
        "        model = lstm_keras(inp_dim, vocab_size, embed_size, num_classes, learn_rate)\n",
        "    elif m_type == \"blstm\":\n",
        "        model = blstm(inp_dim)\n",
        "    elif m_type == \"blstm_attention\":\n",
        "        model = blstm_atten(inp_dim, vocab_size, embed_size, num_classes, learn_rate)\n",
        "    else:\n",
        "        print (\"ERROR: Please specify a correst model\")\n",
        "        return None\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISIiNYbQbEdD",
        "outputId": "35693bdf-be21-44ea-9997-9a3b1be6a1e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement model (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for model\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "pip install model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHuWBEUAfd_h",
        "outputId": "0e460f75-440a-4432-df66-a65b83138410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting preprocessor\n",
            "  Downloading preprocessor-1.1.3.tar.gz (4.2 kB)\n",
            "Building wheels for collected packages: preprocessor\n",
            "  Building wheel for preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for preprocessor: filename=preprocessor-1.1.3-py3-none-any.whl size=4477 sha256=bef416c0ddfacde24b1c34955f100ed0d9523283daf6c7a43a21b3a237d9e4ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/b7/36/aa37256db62b4bfd35a6f1b5536e9ba843f257b79dcbf3d5f1\n",
            "Successfully built preprocessor\n",
            "Installing collected packages: preprocessor\n",
            "Successfully installed preprocessor-1.1.3\n"
          ]
        }
      ],
      "source": [
        "pip install preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX7ympbLfm6G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MqibqESf2b1"
      },
      "outputs": [],
      "source": [
        "pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bwSKp7YZhHAW",
        "outputId": "e8ee8650-834a-4bfd-dad8-c4a6673b2c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting model-import-export\n",
            "  Downloading model_import_export-0.1.2-py3-none-any.whl (7.1 kB)\n",
            "Collecting pytz==2018.5\n",
            "  Downloading pytz-2018.5-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 14.9 MB/s \n",
            "\u001b[?25hCollecting python-dateutil==2.7.3\n",
            "  Downloading python_dateutil-2.7.3-py2.py3-none-any.whl (211 kB)\n",
            "\u001b[K     |████████████████████████████████| 211 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: xlrd==1.1.0 in /usr/local/lib/python3.7/dist-packages (from model-import-export) (1.1.0)\n",
            "Requirement already satisfied: xlwt==1.3.0 in /usr/local/lib/python3.7/dist-packages (from model-import-export) (1.3.0)\n",
            "Collecting numpy==1.15.2\n",
            "  Downloading numpy-1.15.2-cp37-cp37m-manylinux1_x86_64.whl (13.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.8 MB 33.5 MB/s \n",
            "\u001b[?25hCollecting openpyxl==2.5.8\n",
            "  Downloading openpyxl-2.5.8.tar.gz (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 45.4 MB/s \n",
            "\u001b[?25hCollecting pandas==0.23.4\n",
            "  Downloading pandas-0.23.4-cp37-cp37m-manylinux1_x86_64.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 15.5 MB/s \n",
            "\u001b[?25hCollecting jdcal\n",
            "  Downloading jdcal-1.4.1-py2.py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: et_xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl==2.5.8->model-import-export) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil==2.7.3->model-import-export) (1.15.0)\n",
            "Building wheels for collected packages: openpyxl\n",
            "  Building wheel for openpyxl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openpyxl: filename=openpyxl-2.5.8-py2.py3-none-any.whl size=242723 sha256=e36231e1ee321dd4d62d8435d2d5a20ceedd29eec609ecfce2b5ac0e6c57aa0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/6e/91/c2002888ce678f59468d225df9de2231f3d22f825a2fff1157\n",
            "Successfully built openpyxl\n",
            "Installing collected packages: pytz, python-dateutil, numpy, jdcal, pandas, openpyxl, model-import-export\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2022.1\n",
            "    Uninstalling pytz-2022.1:\n",
            "      Successfully uninstalled pytz-2022.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: openpyxl\n",
            "    Found existing installation: openpyxl 3.0.10\n",
            "    Uninstalling openpyxl-3.0.10:\n",
            "      Successfully uninstalled openpyxl-3.0.10\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires numpy>=1.16.0, but you have numpy 1.15.2 which is incompatible.\n",
            "xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.15.2 which is incompatible.\n",
            "xarray 0.20.2 requires pandas>=1.1, but you have pandas 0.23.4 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.15.2 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.15.2 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.15.2 which is incompatible.\n",
            "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.15.2 which is incompatible.\n",
            "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.15.2 which is incompatible.\n",
            "resampy 0.4.0 requires numpy>=1.17, but you have numpy 1.15.2 which is incompatible.\n",
            "pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.15.2 which is incompatible.\n",
            "pymc3 3.11.5 requires pandas>=0.24.0, but you have pandas 0.23.4 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.15.2 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.15.2 which is incompatible.\n",
            "prophet 1.1 requires numpy>=1.15.4, but you have numpy 1.15.2 which is incompatible.\n",
            "prophet 1.1 requires pandas>=1.0.4, but you have pandas 0.23.4 which is incompatible.\n",
            "prophet 1.1 requires python-dateutil>=2.8.0, but you have python-dateutil 2.7.3 which is incompatible.\n",
            "plotnine 0.6.0 requires numpy>=1.16.0, but you have numpy 1.15.2 which is incompatible.\n",
            "plotnine 0.6.0 requires pandas>=0.25.0, but you have pandas 0.23.4 which is incompatible.\n",
            "numba 0.56.0 requires numpy<1.23,>=1.18, but you have numpy 1.15.2 which is incompatible.\n",
            "mizani 0.6.0 requires pandas>=0.25.0, but you have pandas 0.23.4 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.15.2 which is incompatible.\n",
            "jaxlib 0.3.14+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.15.2 which is incompatible.\n",
            "jax 0.3.14 requires numpy>=1.19, but you have numpy 1.15.2 which is incompatible.\n",
            "httpstan 4.6.1 requires numpy<2.0,>=1.16, but you have numpy 1.15.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0, but you have pandas 0.23.4 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.15.2 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.15.2 which is incompatible.\u001b[0m\n",
            "Successfully installed jdcal-1.4.1 model-import-export-0.1.2 numpy-1.15.2 openpyxl-2.5.8 pandas-0.23.4 python-dateutil-2.7.3 pytz-2018.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install model-import-export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYcJ5AX1tNcM",
        "outputId": "2caf7cfc-1e21-4769-92b0-018b67350c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting models==0.9.3\n",
            "  Using cached models-0.9.3.tar.gz (16 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/92/3c/ac1ddde60c02b5a46993bd3c6f4c66a9dbc100059da8333178ce17a22db5/models-0.9.3.tar.gz#sha256=b5aa29c6b57a667cda667dd9fbd33bbd15c14cc285e57dda64f4f4c0fd35e0ae (from https://pypi.org/simple/models/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement models==0.9.3 (from versions: 0.0.3, 0.0.4, 0.0.5, 0.1.0, 0.1.1, 0.2.0, 0.3.0, 0.4.0, 0.7.0, 0.8.0, 0.9.0, 0.9.1, 0.9.2, 0.9.3)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for models==0.9.3\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "pip install models==0.9.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "p7wAZqJTtNfC",
        "outputId": "41ad336a-fe1f-4d03-d9eb-0347a5cc896f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-181ea416d56e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
          ]
        }
      ],
      "source": [
        "models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-8M1NJbtAdf",
        "outputId": "07544118-a5d8-4b50-de00-3a5c163417a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting learn-ml\n",
            "  Downloading learn_ml-0.0.1-py3-none-any.whl (1.1 kB)\n",
            "Installing collected packages: learn-ml\n",
            "Successfully installed learn-ml-0.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install learn-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBCiyXmWviRe",
        "outputId": "37234b2e-f0b0-4927-e5c2-9cbcb236b874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: learn in /usr/local/lib/python3.7/dist-packages (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5JrFv-Jvr4S"
      },
      "outputs": [],
      "source": [
        "import learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7HzS1Xn3Svv"
      },
      "outputs": [],
      "source": [
        "import tensorflow.contrib.tensorrt as trt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LETTT1ZaIvc"
      },
      "outputs": [],
      "source": [
        "pip uninstall tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7py6WAwaJoU"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow==1.13.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERsKvabJaP78"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade tf_slim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uLcVgWaaP-J"
      },
      "outputs": [],
      "source": [
        "import tf_slim as slim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ib2KxvPaAil"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "slim = tf.contrib.slim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01V0SqZhfIhD"
      },
      "outputs": [],
      "source": [
        "pip install preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWhufS5ol6nU",
        "outputId": "918351ad-def0-462e-c37b-a4e30427f578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting learn\n",
            "  Downloading learn-1.0.0.zip (818 bytes)\n",
            "Building wheels for collected packages: learn\n",
            "  Building wheel for learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for learn: filename=learn-1.0.0-py3-none-any.whl size=1255 sha256=329176cfdfb78ceb3c82dfe54233295643defee5ea6e62633ade2998d8dc7b9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/c0/4f/0067cdee2c98b177d60282f4e3cf7191a42ccc400b79580038\n",
            "Successfully built learn\n",
            "Installing collected packages: learn\n",
            "Successfully installed learn-1.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCcAhcELl7By",
        "outputId": "2e0c4160-b946-4a01-a45c-fdad00600363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting preprocessor\n",
            "  Downloading preprocessor-1.1.3.tar.gz (4.2 kB)\n",
            "Building wheels for collected packages: preprocessor\n",
            "  Building wheel for preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for preprocessor: filename=preprocessor-1.1.3-py3-none-any.whl size=4477 sha256=13bdd7e7f9141e783d3e30a63c364d81d2de189b12b0bc10f442878d645d83a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/b7/36/aa37256db62b4bfd35a6f1b5536e9ba843f257b79dcbf3d5f1\n",
            "Successfully built preprocessor\n",
            "Installing collected packages: preprocessor\n",
            "Successfully installed preprocessor-1.1.3\n"
          ]
        }
      ],
      "source": [
        "pip install preprocessor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SANvbi_uwxTg"
      },
      "outputs": [],
      "source": [
        "pip install models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tH_bpu3xlCb"
      },
      "outputs": [],
      "source": [
        "pip install tf-slim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvcTVWa_fxdc"
      },
      "outputs": [],
      "source": [
        "python get-pip.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-hwaB4Gfxh9"
      },
      "outputs": [],
      "source": [
        "pip install tf-slim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqKU0-0VfxkR"
      },
      "outputs": [],
      "source": [
        "TF1.15.0rc3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96kMkqXlfxo5"
      },
      "outputs": [],
      "source": [
        "def get_vocab_processor(model_dir):\n",
        "    vocab_path = os.path.join(model_dir, \"..\", \"vocab\")\n",
        "    #vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
        "    #tflearn import error. tflearn support removed from Tensorflow 2.1.0\n",
        "    import tflearn\n",
        "    vocab_processor = tflearn.data_utils.VocabularyProcessor.restore(filename=vocab_path)\n",
        "    return vocab_processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "7EPAq46gWl_Z",
        "outputId": "872e035b-607f-4a65-a358-fd1d08561519"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-c4b85a833aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from tensorflow.contrib import learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iItYk5R7c0tJ",
        "outputId": "fa085d6b-4ecd-4b3c-cb66-74ca51c7ca78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping python as it is not installed.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB0vjpzRelB6",
        "outputId": "87905950-47b7-419f-c5a0-e2935ebc50a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " This PPA contains more recent Python versions packaged for Ubuntu.\n",
            "\n",
            "Disclaimer: there's no guarantee of timely updates in case of security problems or other issues. If you want to use them in a security-or-otherwise-critical environment (say, on a production server), you do so at your own risk.\n",
            "\n",
            "Update Note\n",
            "===========\n",
            "Please use this repository instead of ppa:fkrull/deadsnakes.\n",
            "\n",
            "Reporting Issues\n",
            "================\n",
            "\n",
            "Issues can be reported in the master issue tracker at:\n",
            "https://github.com/deadsnakes/issues/issues\n",
            "\n",
            "Supported Ubuntu and Python Versions\n",
            "====================================\n",
            "\n",
            "- Ubuntu 18.04 (bionic) Python2.3 - Python 2.6, Python 3.1 - Python 3.5, Python3.7 - Python3.11\n",
            "- Ubuntu 20.04 (focal) Python3.5 - Python3.7, Python3.9 - Python3.11\n",
            "- Ubuntu 22.04 (jammy) Python3.7 - Python3.9, Python3.11\n",
            "- Note: Python2.7 (all), Python 3.6 (bionic), Python 3.8 (focal), Python 3.10 (jammy) are not provided by deadsnakes as upstream ubuntu provides those packages.\n",
            "\n",
            "Why some packages aren't built:\n",
            "- Note: for focal, older python versions require libssl<1.1 so they are not currently built\n",
            "- Note: for jammy, older python versions requre libssl<3 so they are not currently built\n",
            "- If you need these, reach out to asottile to set up a private ppa\n",
            "\n",
            "The packages may also work on other versions of Ubuntu or Debian, but that is not tested or supported.\n",
            "\n",
            "Packages\n",
            "========\n",
            "\n",
            "The packages provided here are loosely based on the debian upstream packages with some modifications to make them more usable as non-default pythons and on ubuntu.  As such, the packages follow debian's patterns and often do not include a full python distribution with just `apt install python#.#`.  Here is a list of packages that may be useful along with the default install:\n",
            "\n",
            "- `python#.#-dev`: includes development headers for building C extensions\n",
            "- `python#.#-venv`: provides the standard library `venv` module\n",
            "- `python#.#-distutils`: provides the standard library `distutils` module\n",
            "- `python#.#-lib2to3`: provides the `2to3-#.#` utility as well as the standard library `lib2to3` module\n",
            "- `python#.#-gdbm`: provides the standard library `dbm.gnu` module\n",
            "- `python#.#-tk`: provides the standard library `tkinter` module\n",
            "\n",
            "Third-Party Python Modules\n",
            "==========================\n",
            "\n",
            "Python modules in the official Ubuntu repositories are packaged to work with the Python interpreters from the official repositories. Accordingly, they generally won't work with the Python interpreters from this PPA. As an exception, pure-Python modules for Python 3 will work, but any compiled extension modules won't.\n",
            "\n",
            "To install 3rd-party Python modules, you should use the common Python packaging tools.  For an introduction into the Python packaging ecosystem and its tools, refer to the Python Packaging User Guide:\n",
            "https://packaging.python.org/installing/\n",
            "\n",
            "Sources\n",
            "=======\n",
            "The package sources are available at:\n",
            "https://github.com/deadsnakes/\n",
            "\n",
            "Nightly Builds\n",
            "==============\n",
            "\n",
            "For nightly builds, see ppa:deadsnakes/nightly https://launchpad.net/~deadsnakes/+archive/ubuntu/nightly\n",
            " More info: https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa\n",
            "Press [ENTER] to continue or Ctrl-c to cancel adding it.\n"
          ]
        }
      ],
      "source": [
        "!add-apt-repository ppa:deadsnakes/ppa\n",
        "!apt-get update\n",
        "!apt-get install python3.5\n",
        "!apt-get install python3.5-dev\n",
        "\n",
        "!wget https://bootstrap.pypa.io/get-pip.py && python3.5 get-pip.py\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv8RGcApc9qU",
        "outputId": "31f7d8a7-6781-477a-a7e0-e97f7707b013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement python==3.6 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for python==3.6\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "pip install python==3.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4KPtiJ5passy",
        "outputId": "929e854c-16ee-4f59-dbda-1aa4befea883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Current Version:- 3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "print(\"User Current Version:-\", sys.version)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et0PI1sIcIs4",
        "outputId": "0032bc76-bc21-436c-98a1-c8f450d869db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3.6 is already the newest version (3.6.9-1~18.04ubuntu1.8).\n",
            "python3.6 set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install python3.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwVL5LRYa2w6",
        "outputId": "68d75cd6-a927-4f84-bef2-f2528ea16be3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement python==3.6 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for python==3.6\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install python==3.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz2rs7OPXcE6",
        "outputId": "0436824f-e5ed-4366-bf23-d3b41806617f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorfllow as it is not installed.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "pip uninstall tensorfllow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTf4TUh5Xl2m"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow==1.14.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKMgAJaEXTjv",
        "outputId": "0bdd615a-a228-42db-89cd-611781ef33f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow.contrib (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tensorflow.contrib\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow.contrib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install preprocessor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG_k0JbHF50h",
        "outputId": "cdbddddb-8a8f-49f5-9c7e-2d07eda65491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: preprocessor in /usr/local/lib/python3.7/dist-packages (1.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBgsFv_3GOnu",
        "outputId": "872acd26-0537-4a26-c30c-e012e0c8ccd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting learn\n",
            "  Downloading learn-1.0.0.zip (818 bytes)\n",
            "Building wheels for collected packages: learn\n",
            "  Building wheel for learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for learn: filename=learn-1.0.0-py3-none-any.whl size=1255 sha256=f620a458fdf78898ca7891d049696c6939bd427efcbc75c79a93841f275a5c93\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/c0/4f/0067cdee2c98b177d60282f4e3cf7191a42ccc400b79580038\n",
            "Successfully built learn\n",
            "Installing collected packages: learn\n",
            "Successfully installed learn-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s27Co4mpbEjk"
      },
      "outputs": [],
      "source": [
        "#from models import get_model\n",
        "import argparse\n",
        "import pickle\n",
        "import string\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import preprocessor as p\n",
        "from collections import Counter\n",
        "import os\n",
        "import learn\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix \n",
        "from tflearn.data_utils import to_categorical, pad_sequences\n",
        "from scipy import stats\n",
        "import tflearn\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHfy96Re69sw"
      },
      "outputs": [],
      "source": [
        "pip uninstall tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvT74vNW69vk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a2b7b43-d644-455b-e356-b91d1277952f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 24 kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.47.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 58.5 MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.14.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=5f29283c4a2957bca0fdf4cc01a9f3720d7aa0e0637864b9c3bf0cf0203c9446\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install tensorflow==1.15.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucZLpma1690A"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG2Yutnj693b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTVoraaQbEoi"
      },
      "outputs": [],
      "source": [
        "def load_data(filename):\n",
        "    print(\"Loading data from file: \" + filename)\n",
        "    data = pickle.load(open(filename, 'rb'))\n",
        "    x_text = []\n",
        "    labels = [] \n",
        "    for i in range(len(data)):\n",
        "        if(HASH_REMOVE):\n",
        "            x_text.append(p.tokenize((data[i]['text']).encode('utf-8')))\n",
        "        else:\n",
        "            x_text.append(data[i]['text'])\n",
        "        labels.append(data[i]['label'])\n",
        "    return x_text,labels\n",
        "\n",
        "def get_filename(dataset):\n",
        "    global NUM_CLASSES, HASH_REMOVE\n",
        "    if(dataset==\"twitter\"):\n",
        "        NUM_CLASSES = 3\n",
        "        HASH_REMOVE = True\n",
        "        filename = \"/content/drive/MyDrive/Main_Project/Detecting-Cyberbullying-Across-SMPs-master/data 2/twitter_data.CSV\"\n",
        "    elif(dataset==\"formspring\"):\n",
        "        NUM_CLASSES = 2\n",
        "        filename = \"/content/drive/MyDrive/Main_Project/Detecting-Cyberbullying-Across-SMPs-master/data 2/formspring_data.csv\"\n",
        "    elif(dataset==\"wiki\"):\n",
        "        NUM_CLASSES = 2\n",
        "        filename = \"/content/drive/MyDrive/Main_Project/Detecting-Cyberbullying-Across-SMPs-master/data 2/wiki_data.csv\"\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJcrtvmabEtU"
      },
      "outputs": [],
      "source": [
        "def get_embedding_weights(filename, sep):\n",
        "    embed_dict = {}\n",
        "    file = open(filename,'r')\n",
        "    for line in file.readlines():\n",
        "        row = line.strip().split(sep)\n",
        "        embed_dict[row[0]] = row[1:]\n",
        "    print('Loaded from file: ' + str(filename))\n",
        "    file.close()\n",
        "    return embed_dict\n",
        "\n",
        "def map_embedding_weights(embed, vocab, embed_size):\n",
        "    vocab_size = len(vocab)\n",
        "    embeddingWeights = np.zeros((vocab_size , embed_size))\n",
        "    n = 0\n",
        "    words_missed = []\n",
        "    for k, v in vocab.iteritems():\n",
        "        try:\n",
        "            embeddingWeights[v] = embed[k]\n",
        "        except:\n",
        "            n += 1\n",
        "            words_missed.append(k)\n",
        "            pass\n",
        "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
        "    return embeddingWeights\n",
        "\n",
        "def get_embeddings_dict(vector_type, emb_dim):\n",
        "    if vector_type == 'sswe':\n",
        "        emb_dim==50\n",
        "        sep = '\\t'\n",
        "        vector_file = 'word_vectors/sswe-u.txt'\n",
        "    elif vector_type ==\"glove\":\n",
        "        sep = ' '\n",
        "        if data == \"wiki\":\n",
        "            vector_file = 'word_vectors/glove.6B.' + str(emb_dim) + 'd.txt'\n",
        "        else:\n",
        "            vector_file = 'word_vectors/glove.twitter.27B.' + str(emb_dim) + 'd.txt'\n",
        "    else:\n",
        "        print (\"ERROR: Please specify a correst model or SSWE cannot be loaded with embed size of: \" + str(emb_dim) )\n",
        "        return None\n",
        "    \n",
        "    embed = get_embedding_weights(vector_file, sep)\n",
        "    return embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXY2fqs9mFD6"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, testX, testY):\n",
        "    temp = model.predict(testX)\n",
        "    y_pred  = np.argmax(temp, 1)\n",
        "    y_true = np.argmax(testY, 1)\n",
        "    precision = metrics.precision_score(y_true, y_pred, average=None)\n",
        "    recall = metrics.recall_score(y_true, y_pred, average=None)\n",
        "    f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
        "    print(\"Precision: \" + str(precision) + \"\\n\")\n",
        "    print(\"Recall: \" + str(recall) + \"\\n\")\n",
        "    print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\":: Classification Report\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    return precision, recall, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuhwpKbmmFHE"
      },
      "outputs": [],
      "source": [
        "def dump_learned_embedding(data, model_type, vector_type, embed_size, embed, vocab_processor):\n",
        "    vocab = vocab_processor.vocabulary_._mapping\n",
        "    vocab_size = len(vocab)\n",
        "    embedDict = {}\n",
        "    n = 0\n",
        "    words_missed = []\n",
        "    for k, v in vocab.iteritems():\n",
        "        try:\n",
        "            embeddingDict[v] = embed[k]\n",
        "        except:\n",
        "            n += 1\n",
        "            words_missed.append(k)\n",
        "            pass\n",
        "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
        "    \n",
        "    filename = output_folder_name + data + \"_\" + model_type + \"_\" + vector_type + \"_\" + embed_size + \".pkl\"\n",
        "    with open(filename, 'wb') as handle:\n",
        "        pickle.dump(embedDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0DW_9L2qMOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e262aeec-5d79-4db4-a398-dfbd6530af26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvJoFhCpmFJV"
      },
      "outputs": [],
      "source": [
        "def get_train_test(data, x_text, labels):\n",
        "    \n",
        "    X_train, X_test, Y_train, Y_test = train_test_split( x_text, labels, random_state=42, test_size=0.10)\n",
        "    \n",
        "    post_length = np.array([len(x.split(\" \")) for x in x_text])\n",
        "    if(data != \"twitter\"):\n",
        "        max_document_length = int(np.percentile(post_length, 95))\n",
        "    else:\n",
        "        max_document_length = max(post_length)\n",
        "    print(\"Document length : \" + str(max_document_length))\n",
        "    \n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, MAX_FEATURES)\n",
        "    vocab_processor = vocab_processor.fit(x_text)\n",
        "\n",
        "    trainX = np.array(list(vocab_processor.transform(X_train)))\n",
        "    testX = np.array(list(vocab_processor.transform(X_test)))\n",
        "    \n",
        "    trainY = np.asarray(Y_train)\n",
        "    testY = np.asarray(Y_test)\n",
        "        \n",
        "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
        "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
        "\n",
        "    trainY = to_categorical(trainY, nb_classes=NUM_CLASSES)\n",
        "    testY = to_categorical(testY, nb_classes=NUM_CLASSES)\n",
        "    \n",
        "    data_dict = {\n",
        "        \"data\": data,\n",
        "        \"trainX\" : trainX,\n",
        "        \"trainY\" : trainY,\n",
        "        \"testX\" : testX,\n",
        "        \"testY\" : testY,\n",
        "        \"vocab_processor\" : vocab_processor\n",
        "    }\n",
        "    \n",
        "    return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWdhxTw8mFK8"
      },
      "outputs": [],
      "source": [
        "def return_data(data_dict):\n",
        "    return data_dict[\"data\"], data_dict[\"trainX\"], data_dict[\"trainY\"], data_dict[\"testX\"], data_dict[\"testY\"], data_dict[\"vocab_processor\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7aWH2_amFNS"
      },
      "outputs": [],
      "source": [
        "def shuffle_weights(model, weights=None):\n",
        "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
        "    This is a fast approximation of re-initializing the weights of a model.\n",
        "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
        "      (i.e., the weights have the same distribution along each dimension).\n",
        "    :param Model model: Modify the weights of the given model.\n",
        "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
        "      If `None`, permute the model's current weights.\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        weights = model.get_weights()\n",
        "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
        "    # Faster, but less random: only permutes along the first dimension\n",
        "    # weights = [np.random.permutation(w) for w in weights]\n",
        "    model.set_weights(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6xOh_mGmFPV"
      },
      "outputs": [],
      "source": [
        "def train(data_dict, model_type, vector_type, embed_size, dump_embeddings=False):\n",
        "\n",
        "    data, trainX, trainY, testX, testY, vocab_processor = return_data(data_dict)\n",
        "    \n",
        "    vocab_size = len(vocab_processor.vocabulary_)\n",
        "    print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
        "    vocab = vocab_processor.vocabulary_._mapping\n",
        "    \n",
        "    print(\"Running Model: \" + model_type + \" with word vector initiliazed with \" + vector_type + \" word vectors.\")\n",
        "    model = get_model(model_type, trainX.shape[1], vocab_size, embed_size, NUM_CLASSES, LEARN_RATE)\n",
        "\n",
        "    initial_weights = model.get_weights()\n",
        "    shuffle_weights(model, initial_weights)\n",
        "    \n",
        "    if(model_type == 'cnn'):\n",
        "        if(vector_type!=\"random\"):\n",
        "            print(\"Word vectors used: \" + vector_type)\n",
        "            embeddingWeights = tflearn.get_layer_variables_by_name('EmbeddingLayer')[0]\n",
        "            model.set_weights(embeddingWeights, map_embedding_weights(get_embeddings_dict(vector_type, embed_size), vocab, embed_size))\n",
        "            model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
        "        else:\n",
        "            model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
        "    else:\n",
        "        if(vector_type!=\"random\"):\n",
        "            print(\"Word vectors used: \" + vector_type)\n",
        "            model.layers[0].set_weights([map_embedding_weights(get_embeddings_dict(vector_type, embed_size), vocab, embed_size)])\n",
        "            model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
        "                  verbose=1)\n",
        "        else:\n",
        "            model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
        "                  verbose=1)\n",
        "            \n",
        "    if (dump_embeddings==True):\n",
        "        if(model_type == 'cnn'):\n",
        "            embeddingWeights = tflearn.get_layer_variables_by_name('EmbeddingLayer')[0]\n",
        "        else:\n",
        "            embed = model.layers[0].get_weights()[0]\n",
        "    \n",
        "        embed_filename = output_folder_name + data + \"_\" + model_type + \"_\" + vector_type + \"_\" + str(embed_size) + \".pkl\"\n",
        "        embed.dump(embed_filename)\n",
        "        \n",
        "        vocab_filename = output_folder_name + data + \"_\" + model_type + \"_\" + vector_type + \"_\" + str(embed_size) + \"_dict.json\"\n",
        "        reverse_vocab_filename = output_folder_name + data + \"_\" + model_type + \"_\" + vector_type + \"_\" + str(embed_size) + \"_reversedict.json\"\n",
        "        \n",
        "        with open(vocab_filename, 'w') as fp:\n",
        "            json.dump(vocab_processor.vocabulary_._mapping, fp)\n",
        "        with open(reverse_vocab_filename, 'w') as fp:\n",
        "            json.dump(vocab_processor.vocabulary_._reverse_mapping, fp)\n",
        "    \n",
        "    return  evaluate_model(model, testX, testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzApCPysmFRb"
      },
      "outputs": [],
      "source": [
        "def print_scores(precision_scores, recall_scores, f1_scores):\n",
        "    for i in range(NUM_CLASSES):\n",
        "        print(\"\\nPrecision Class %d (avg): %0.3f (+/- %0.3f)\" % (i, precision_scores[:, i].mean(), precision_scores[:, i].std() * 2))\n",
        "        print( \"\\nRecall Class %d (avg): %0.3f (+/- %0.3f)\" % (i, recall_scores[:, i].mean(), recall_scores[:, i].std() * 2))\n",
        "        print( \"\\nF1 score Class %d (avg): %0.3f (+/- %0.3f)\" % (i, f1_scores[:, i].mean(), f1_scores[:, i].std() * 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ7W5zRgmFTg"
      },
      "outputs": [],
      "source": [
        "def get_data(data, oversampling_rate):\n",
        "    \n",
        "    x_text, labels = load_data(get_filename(data)) \n",
        " \n",
        "    if(data==\"twitter\"):\n",
        "        NUM_CLASSES = 3\n",
        "        dict1 = {'racism':2,'sexism':1,'none':0}\n",
        "        labels = [dict1[b] for b in labels]\n",
        "        \n",
        "        racism = [i for i in range(len(labels)) if labels[i]==2]\n",
        "        sexism = [i for i in range(len(labels)) if labels[i]==1]\n",
        "        x_text = x_text + [x_text[x] for x in racism]*(oversampling_rate-1)+ [x_text[x] for x in sexism]*(oversampling_rate-1)\n",
        "        labels = labels + [2 for i in range(len(racism))]*(oversampling_rate-1) + [1 for i in range(len(sexism))]*(oversampling_rate-1)\n",
        "    \n",
        "    else:  \n",
        "        NUM_CLASSES = 2\n",
        "        bully = [i for i in range(len(labels)) if labels[i]==1]\n",
        "        x_text = x_text + [x_text[x] for x in bully]*(oversampling_rate-1)\n",
        "        labels = list(labels) + [1 for i in range(len(bully))]*(oversampling_rate-1)\n",
        "\n",
        "    print(\"Counter after oversampling\")\n",
        "    from collections import Counter\n",
        "    print(Counter(labels))\n",
        "    \n",
        "    filter_data = []\n",
        "    for text in x_text:\n",
        "        filter_data.append(\"\".join(l for l in text if l not in string.punctuation))\n",
        "        \n",
        "    return x_text, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G6BYw3SmFVm"
      },
      "outputs": [],
      "source": [
        "models = [ 'cnn', 'lstm', 'blstm', 'blstm_attention']\n",
        "word_vectors = [\"random\", \"glove\" ,\"sswe\"]\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "MAX_FEATURES = 2\n",
        "NUM_CLASSES = None\n",
        "DROPOUT = 0.25\n",
        "LEARN_RATE = 0.01\n",
        "HASH_REMOVE = None\n",
        "output_folder_name = \"results/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBj1VV-emFaC"
      },
      "outputs": [],
      "source": [
        "def run_model(data, oversampling_rate, model_type, vector_type, embed_size):    \n",
        "    x_text, labels = get_data(data, oversampling_rate)\n",
        "    data_dict = get_train_test(data,  x_text, labels)\n",
        "    precision, recall, f1_score = train(data_dict, model_type, vector_type, embed_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohfZyARc1TTz"
      },
      "outputs": [],
      "source": [
        "import sklearn as preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMvRyaf-2op_",
        "outputId": "9cd4ae09-0bf5-41d2-9e84-9254449ac37c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting preprocessing\n",
            "  Downloading preprocessing-0.1.13-py3-none-any.whl (349 kB)\n",
            "\u001b[K     |████████████████████████████████| 349 kB 13.4 MB/s \n",
            "\u001b[?25hCollecting sphinx-rtd-theme==0.2.4\n",
            "  Downloading sphinx_rtd_theme-0.2.4-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 40.0 MB/s \n",
            "\u001b[?25hCollecting nltk==3.2.4\n",
            "  Downloading nltk-3.2.4.tar.gz (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.2.4->preprocessing) (1.15.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.2.4-py3-none-any.whl size=1367722 sha256=72ffaf94a830c90c5ae2f140a68e955f3f3bd7e30c20a63e6e4731bc6e971660\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/5e/9e/4cb46185f2a16c60e6fc524372ba7fef89ce3347734c8798b6\n",
            "Successfully built nltk\n",
            "Installing collected packages: sphinx-rtd-theme, nltk, preprocessing\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed nltk-3.2.4 preprocessing-0.1.13 sphinx-rtd-theme-0.2.4\n"
          ]
        }
      ],
      "source": [
        "pip install preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "okAKb419nrd0",
        "outputId": "c822aa27-71df-44fa-e943-0e7aa5b7bca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from file: /content/drive/MyDrive/Main_Project/Detecting-Cyberbullying-Across-SMPs-master/data 2/formspring_data.csv\n",
            "Counter after oversampling\n",
            "Counter({0: 11997, 1: 2328})\n",
            "Document length : 62\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-16b2ef283c99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0membed_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-56-d33cf078dbd4>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(data, oversampling_rate, model_type, vector_type, embed_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moversampling_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moversampling_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mx_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-a17787d280cd>\u001b[0m in \u001b[0;36mget_train_test\u001b[0;34m(data, x_text, labels)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Document length : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_document_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mvocab_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVocabularyProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_document_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_FEATURES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvocab_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'learn' has no attribute 'preprocessing'"
          ]
        }
      ],
      "source": [
        "data = \"formspring\"\n",
        "model_type = \"blstm_attention\"\n",
        "vector_type = \"random\"\n",
        "\n",
        "for embed_size in [25, 50, 100, 200]:\n",
        "  run_model(data, 3, model_type, vector_type, embed_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTbDBFBJDWXI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFjCOtUcnrgY"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/usr/local/bin/python2.7')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os, sys, getopt, pickle, csv, sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, make_scorer, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "from sklearn.ensemble  import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.utils import shuffle\n",
        "from textblob import TextBlob\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from collections import Counter\n",
        "import argparse\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import roc_auc_score    \n",
        "import preprocessor as p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSZ5NbMlDhFV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIVlmGvpnriv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0TFqqovnrlI"
      },
      "outputs": [],
      "source": [
        "models = [ 'svm', 'naive', 'lr', 'random_forest']\n",
        "NO_OF_FOLDS = 5\n",
        "MODEL_TYPE = \"all\"\n",
        "HASH_REMOVE = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL8dtFLIEBzR"
      },
      "outputs": [],
      "source": [
        "def load_data(filename):\n",
        "    data = pickle.load(open(filename, 'rb'))\n",
        "    x_text = []\n",
        "    labels = []\n",
        "    for i in range(len(data)):\n",
        "        if(HASH_REMOVE):\n",
        "            x_text.append(p.tokenize((data[i]['text']).encode('utf-8')))\n",
        "        else:\n",
        "            x_text.append(data[i]['text'])\n",
        "        labels.append(data[i]['label'])\n",
        "    return x_text,labels\n",
        "\n",
        "def get_filename(dataset):\n",
        "    global N_CLASS, HASH_REMOVE\n",
        "    if(dataset==\"twitter\"):\n",
        "        filename = \"/content/drive/MyDrive/Main_Project/Detecting-Cyberbullying-Across-SMPs-master/data 2/twitter_data.CSV\"\n",
        "        N_CLASS = 3\n",
        "        HASH_REMOVE = False\n",
        "    elif(dataset==\"formspring\"):\n",
        "        N_CLASS = 2\n",
        "        filename = \"/content/drive/MyDrive/Main_Project/Detecting-Cyberbullying-Across-SMPs-master/data 2/formspring_data.csv\"\n",
        "        HASH_REMOVE = False\n",
        "    elif(dataset==\"wiki\"):\n",
        "        N_CLASS = 2\n",
        "        filename = \"/content/drive/MyDrive/Main_Project/Detecting-Cyberbullying-Across-SMPs-master/data 2/wiki_data.csv\"\n",
        "        HASH_REMOVE = False\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN0o_ZupEB2K"
      },
      "outputs": [],
      "source": [
        "def get_scores(y_true, y_pred):\n",
        "#     if(data==\"wiki\"):\n",
        "#         auc = roc_auc_score(y_true,y_pred)\n",
        "#         print('Test ROC AUC: %.3f' %auc)\n",
        "#     print(\":: Confusion Matrix\")\n",
        "#     print(confusion_matrix(y_true, y_pred))\n",
        "#     print(\":: Classification Report\")\n",
        "#     print(classification_report(y_true, y_pred))\n",
        "    return np.array([ \n",
        "            precision_score(y_true, y_pred, average=None), \n",
        "            recall_score(y_true, y_pred,  average=None),\n",
        "            f1_score(y_true, y_pred, average=None)])\n",
        "    \n",
        "def print_scores(scores):\n",
        "    for i in range(N_CLASS):\n",
        "        if(i!=0):\n",
        "            print (\"Precision Class %d (avg): %0.3f (+/- %0.3f)\" % (i,scores[:, i].mean(), scores[:, i].std() * 2))\n",
        "            print (\"Recall Class %d (avg): %0.3f (+/- %0.3f)\" % (i,scores[:,  N_CLASS+i].mean(), scores[:,N_CLASS+i].std() * 2))\n",
        "            print (\"F1_score Class %d (avg): %0.3f (+/- %0.3f)\" % (i,scores[:, N_CLASS*2+i].mean(), scores[:,  N_CLASS*2+i].std() * 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uenW__JEB5T"
      },
      "outputs": [],
      "source": [
        "def classification_model(X, Y, model_type):\n",
        "    X, Y = shuffle(X, Y, random_state=42)\n",
        "    print (\"Model Type:\", model_type)\n",
        "    kf = KFold(n_splits=NO_OF_FOLDS)\n",
        "    scores = []\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        Y = np.asarray(Y)\n",
        "        model = get_model(model_type)\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = Y[train_index], Y[test_index]\n",
        "        model.fit(X_train,y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        curr_scores = get_scores(y_test, y_pred)\n",
        "        scores.append(np.hstack(curr_scores))\n",
        "    print_scores(np.array(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7OQuxWXEOYP"
      },
      "outputs": [],
      "source": [
        "def get_model(m_type):\n",
        "    if m_type == 'lr':\n",
        "        logreg = LogisticRegression(class_weight=\"balanced\")\n",
        "    elif m_type == 'naive':\n",
        "        logreg =  MultinomialNB()\n",
        "    elif m_type == \"random_forest\":\n",
        "        logreg = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
        "    elif m_type == \"svm\":\n",
        "        logreg = LinearSVC(class_weight=\"balanced\")\n",
        "    else:\n",
        "        print (\"ERROR: Please specify a correst model\")\n",
        "        return None\n",
        "    return logreg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Sit9YsTEOed"
      },
      "outputs": [],
      "source": [
        "def train(x_text, labels, MODEL_TYPE):\n",
        "    \n",
        "    if(WORD):\n",
        "        print(\"Using word based features\")\n",
        "        bow_transformer = CountVectorizer(analyzer=\"word\",max_features = 10000,stop_words='english').fit(x_text)\n",
        "        comments_bow = bow_transformer.transform(x_text)\n",
        "        tfidf_transformer = TfidfTransformer(norm = 'l2').fit(comments_bow)\n",
        "        comments_tfidf = tfidf_transformer.transform(comments_bow)\n",
        "        features = comments_tfidf\n",
        "    else: \n",
        "        print(\"Using char n-grams based features\")\n",
        "        bow_transformer = CountVectorizer(max_features = 1000, ngram_range = (1,2)).fit(x_text)\n",
        "        comments_bow = bow_transformer.transform(x_text)\n",
        "        tfidf_transformer = TfidfTransformer(norm = 'l2').fit(comments_bow)\n",
        "        comments_tfidf = tfidf_transformer.transform(comments_bow)\n",
        "        features = comments_tfidf\n",
        "    \n",
        "    if(data == \"twitter\"):\n",
        "        dict1 = {'racism':0,'sexism':1,'none':2}\n",
        "        labels = np.array([dict1[b] for b in labels])\n",
        "    \n",
        "    from collections import Counter\n",
        "    print(Counter(labels))\n",
        "    \n",
        "    if(MODEL_TYPE != \"all\"):\n",
        "        classification_model(features, labels, MODEL_TYPE)\n",
        "    else:\n",
        "        for model_type in models:\n",
        "            classification_model(features, labels, model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3sHNJ3tEOhU"
      },
      "outputs": [],
      "source": [
        "data = \"formspring\"\n",
        "x_text, labels = load_data(get_filename(data)) \n",
        "print (\"Data loaded!\")\n",
        "train(x_text, labels, MODEL_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLri-3kuEOkL"
      },
      "outputs": [],
      "source": [
        "data = \"formspring\"\n",
        "WORD = True\n",
        "x_text, labels = load_data(get_filename(data)) \n",
        "print (\"Data loaded!\")\n",
        "train(x_text, labels, MODEL_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eIiXp7sEOmz"
      },
      "outputs": [],
      "source": [
        "data = \"twitter\"\n",
        "WORD = True\n",
        "x_text, labels = load_data(get_filename(data)) \n",
        "print (\"Data loaded!\")\n",
        "train(x_text, labels, MODEL_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybJUQsGG7iwc"
      },
      "outputs": [],
      "source": [
        "data = \"twitter\"\n",
        "WORD = False\n",
        "x_text, labels = load_data(get_filename(data)) \n",
        "print (\"Data loaded!\")\n",
        "train(x_text, labels, MODEL_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHInRcIxEOpt"
      },
      "outputs": [],
      "source": [
        "data = \"wiki\"\n",
        "x_text, labels = load_data(get_filename(data)) \n",
        "print (\"Data loaded!\")\n",
        "train(x_text, labels, MODEL_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MSFnT2TQhFp"
      },
      "outputs": [],
      "source": [
        "pip install model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_U_GOs9EOsx"
      },
      "outputs": [],
      "source": [
        "#from models import get_model\n",
        "import argparse\n",
        "import pickle\n",
        "import string\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import preprocessor as p\n",
        "from collections import Counter\n",
        "import os\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix \n",
        "from tensorflow.contrib import learn\n",
        "from tflearn.data_utils import to_categorical, pad_sequences\n",
        "from scipy import stats\n",
        "import tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_nUw-k5SysS"
      },
      "outputs": [],
      "source": [
        "import learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQldPSYdEOvp"
      },
      "outputs": [],
      "source": [
        "models = [ 'cnn', 'lstm', 'blstm', 'blstm_attention']\n",
        "word_vectors = [\"random\", \"glove\" ,\"sswe\"]\n",
        "EMBED_SIZE = 50\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 128\n",
        "MAX_FEATURES = 2\n",
        "NUM_CLASSES = 2\n",
        "DROPOUT = 0.25\n",
        "LEARN_RATE = 0.01\n",
        "HASH_REMOVE=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR5LiM0kEOyj"
      },
      "outputs": [],
      "source": [
        "def load_data(filename):\n",
        "    print(\"Loading data from file: \" + filename)\n",
        "    data = pickle.load(open(filename, 'rb'))\n",
        "    x_text = []\n",
        "    labels = [] \n",
        "    for i in range(len(data)):\n",
        "        if(HASH_REMOVE):\n",
        "            x_text.append(p.tokenize((data[i]['text']).encode('utf-8')))\n",
        "        else:\n",
        "            x_text.append(data[i]['text'])\n",
        "        labels.append(data[i]['label'])\n",
        "    return x_text,labels\n",
        "\n",
        "def get_filename(dataset):\n",
        "    global HASH_REMOVE\n",
        "    if(dataset==\"twitter\"):\n",
        "        HASH_REMOVE = True\n",
        "        EPOCHS = 10\n",
        "        BATCH_SIZE = 128\n",
        "        MAX_FEATURES = 2\n",
        "        filename = \"/content/drive/MyDrive/Main_Project/Detecting-Cyberbullying-Across-SMPs-master/data 2/twitter_data.CSV\"\n",
        "    elif(dataset==\"formspring\"):\n",
        "        HASH_REMOVE = False\n",
        "        EPOCHS = 10\n",
        "        BATCH_SIZE = 128\n",
        "        MAX_FEATURES = 2\n",
        "        filename = \"/content/drive/MyDrive/Main_Project/Detecting-Cyberbullying-Across-SMPs-master/data 2/formspring_data.csv\"\n",
        "    elif(dataset==\"wiki\"):\n",
        "        HASH_REMOVE = False\n",
        "        EPOCHS = 5\n",
        "        BATCH_SIZE = 512\n",
        "        MAX_FEATURES = 5\n",
        "        filename = \"/content/drive/MyDrive/Main_Project/Detecting-Cyberbullying-Across-SMPs-master/data 2/wiki_data.csv\"\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auXfJRbaEO1a"
      },
      "outputs": [],
      "source": [
        "def get_embedding_weights(filename, sep):\n",
        "    embed_dict = {}\n",
        "    file = open(filename,'r')\n",
        "    for line in file.readlines():\n",
        "        row = line.strip().split(sep)\n",
        "        embed_dict[row[0]] = row[1:]\n",
        "    print('Loaded from file: ' + str(filename))\n",
        "    file.close()\n",
        "    return embed_dict\n",
        "\n",
        "def map_embedding_weights(embed, vocab, embed_size):\n",
        "    vocab_size = len(vocab)\n",
        "    embeddingWeights = np.zeros((vocab_size , embed_size))\n",
        "    n = 0\n",
        "    words_missed = []\n",
        "    for k, v in vocab.iteritems():\n",
        "        try:\n",
        "            embeddingWeights[v] = embed[k]\n",
        "        except:\n",
        "            n += 1\n",
        "            words_missed.append(k)\n",
        "            pass\n",
        "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
        "    return embeddingWeights\n",
        "\n",
        "def get_embeddings_dict(vector_type, emb_dim, data):\n",
        "    if vector_type == 'sswe':\n",
        "        emb_dim==50\n",
        "        sep = '\\t'\n",
        "        vector_file = 'word_vectors/sswe-u.txt'\n",
        "    elif vector_type ==\"glove\":\n",
        "        sep = ' '\n",
        "        if data == \"wiki\":\n",
        "            vector_file = 'word_vectors/glove.6B.' + str(emb_dim) + 'd.txt'\n",
        "        else:\n",
        "            vector_file = 'word_vectors/glove.twitter.27B.' + str(emb_dim) + 'd.txt'\n",
        "    else:\n",
        "        print (\"ERROR: Please specify a correst model or SSWE cannot be loaded with embed size of: \" + str(emb_dim) )\n",
        "        return None\n",
        "    \n",
        "    embed = get_embedding_weights(vector_file, sep)\n",
        "    return embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyE8O9OzQ5Ss"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, testX, testY):\n",
        "    temp = model.predict(testX)\n",
        "    y_pred  = np.argmax(temp, 1)\n",
        "    y_true = np.argmax(testY, 1)\n",
        "    precision = metrics.precision_score(y_true, y_pred, average=None)\n",
        "    recall = metrics.recall_score(y_true, y_pred, average=None)\n",
        "    f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
        "    print(\"Precision: \" + str(precision) + \"\\n\")\n",
        "    print(\"Recall: \" + str(recall) + \"\\n\")\n",
        "    print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(\":: Classification Report\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    return precision, recall, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TuwFpvaQ5Vw"
      },
      "outputs": [],
      "source": [
        "def print_scores(precision_scores, recall_scores, f1_scores):\n",
        "    for i in range(NUM_CLASSES):\n",
        "        print(\"\\nPrecision Class %d (avg): %0.3f (+/- %0.3f)\" % (i, precision_scores[:, i].mean(), precision_scores[:, i].std() * 2))\n",
        "        print( \"\\nRecall Class %d (avg): %0.3f (+/- %0.3f)\" % (i, recall_scores[:, i].mean(), recall_scores[:, i].std() * 2))\n",
        "        print( \"\\nF1 score Class %d (avg): %0.3f (+/- %0.3f)\" % (i, f1_scores[:, i].mean(), f1_scores[:, i].std() * 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxEjpRPkQ5Yn"
      },
      "outputs": [],
      "source": [
        "def get_data(data, oversampling_rate):\n",
        "    \n",
        "    x_text, labels = load_data(get_filename(data)) \n",
        " \n",
        "    if(data==\"twitter\"):\n",
        "        dict1 = {'racism':1,'sexism':1,'none':0} #Transfer learning only two classes\n",
        "        labels = [dict1[b] for b in labels]\n",
        "        \n",
        "        racism = [i for i in range(len(labels)) if labels[i]==2]\n",
        "        sexism = [i for i in range(len(labels)) if labels[i]==1]\n",
        "        x_text = x_text + [x_text[x] for x in racism]*(oversampling_rate-1)+ [x_text[x] for x in sexism]*(oversampling_rate-1)\n",
        "        labels = labels + [2 for i in range(len(racism))]*(oversampling_rate-1) + [1 for i in range(len(sexism))]*(oversampling_rate-1)\n",
        "    \n",
        "    else:\n",
        "        \n",
        "        NUM_CLASSES = 2\n",
        "        bully = [i for i in range(len(labels)) if labels[i]==1]\n",
        "        x_text = x_text + [x_text[x] for x in bully]*(oversampling_rate-1)\n",
        "        labels = list(labels) + [1 for i in range(len(bully))]*(oversampling_rate-1)\n",
        "\n",
        "    print(\"Counter after oversampling\")\n",
        "    from collections import Counter\n",
        "    print(Counter(labels))\n",
        "    \n",
        "    filter_data = []\n",
        "    for text in x_text:\n",
        "        filter_data.append(\"\".join(l for l in text if l not in string.punctuation))\n",
        "        \n",
        "    return x_text, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AVdNZp23UId"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VqH7wgl3HeD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ob3gVry29Yd"
      },
      "outputs": [],
      "source": [
        "import tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMj9VKS1Q5bK"
      },
      "outputs": [],
      "source": [
        "def train(data, x_text, labels, model_type, vector_type, embed_size, max_document_length=None):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split( x_text, labels, random_state=121, test_size=0.10)\n",
        "    \n",
        "    if(max_document_length==None):\n",
        "        post_length = np.array([len(x.split(\" \")) for x in x_text])\n",
        "        if(data != \"twitter\"):\n",
        "            max_document_length = int(np.percentile(post_length, 95))\n",
        "        else:\n",
        "            max_document_length = max(post_length)\n",
        "        print(\"Document length : \" + str(max_document_length))\n",
        "    \n",
        "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, MAX_FEATURES)\n",
        "    vocab_processor = vocab_processor.fit(x_text)\n",
        "\n",
        "    trainX = np.array(list(vocab_processor.transform(X_train)))\n",
        "    testX = np.array(list(vocab_processor.transform(X_test)))\n",
        "\n",
        "    vocab_size = len(vocab_processor.vocabulary_)\n",
        "    print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
        "\n",
        "    vocab = vocab_processor.vocabulary_._mapping\n",
        "    \n",
        "    trainY = np.asarray(Y_train)\n",
        "    testY = np.asarray(Y_test)\n",
        "        \n",
        "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
        "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
        "\n",
        "    trainY = to_categorical(trainY, nb_classes=NUM_CLASSES)\n",
        "    testY = to_categorical(testY, nb_classes=NUM_CLASSES)\n",
        "\n",
        "    print(\"Running Model: \" + model_type + \" with word vector initiliazed with \" + vector_type + \" word vectors.\")\n",
        "    model = get_model(model_type, trainX.shape[1], vocab_size, embed_size, NUM_CLASSES, LEARN_RATE)\n",
        "\n",
        "    if(model_type == 'cnn'):\n",
        "        if(vector_type!=\"random\"):\n",
        "            print(\"Word vectors used: \" + vector_type)\n",
        "            embeddingWeights = tflearn.get_layer_variables_by_name('EmbeddingLayer')[0]\n",
        "            model.set_weights(embeddingWeights, map_embedding_weights(get_embeddings_dict(vector_type, embed_size, data), vocab, embed_size))\n",
        "            model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
        "        else:\n",
        "            model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
        "    else:\n",
        "        if(vector_type!=\"random\"):\n",
        "            print(\"Word vectors used: \" + vector_type)\n",
        "            model.layers[0].set_weights([map_embedding_weights(get_embeddings_dict(vector_type, embed_size, data), vocab, embed_size)])\n",
        "            model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
        "                  verbose=1)\n",
        "        else:\n",
        "            model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
        "                  verbose=1)\n",
        "            \n",
        "    precision, recall, f1_score = evaluate_model(model, testX, testY)\n",
        "    \n",
        "    model_dict = {\n",
        "        \"model\": model,\n",
        "        \"testX\": testX,\n",
        "        \"testY\": testY,\n",
        "        \"trainX\" : trainX,\n",
        "        \"trainY\" : trainY,\n",
        "        \"vocab\": vocab_processor,\n",
        "        \"length\": max_document_length,\n",
        "        \"data\": data\n",
        "    }\n",
        "    \n",
        "    return model_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMEBJw_EQ5eQ"
      },
      "outputs": [],
      "source": [
        "def transfer_learning_1(dict_1, dict_2, model_type=None, vector_type=None, embed_size=None):\n",
        "    \n",
        "    model = dict_1[\"model\"]\n",
        "    length = dict_1[\"length\"]\n",
        "    vocab1 = dict_1[\"vocab\"]\n",
        "    vocab2 = dict_2[\"vocab\"]\n",
        "    testX = dict_2[\"testX\"]\n",
        "    testY = dict_2[\"testY\"]\n",
        "    \n",
        "    temp = list(vocab2.reverse(testX))\n",
        "    testX  = np.array(list(vocab1.transform(temp)))\n",
        "    testX = pad_sequences(testX, maxlen=length, value=0.)\n",
        "    \n",
        "    evaluate_model(model, testX, testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSepSCofQ5gK"
      },
      "outputs": [],
      "source": [
        "def map_embedding_trained_weights(embed, vocab_1, vocab_2):\n",
        "    vocab_size = len(vocab_2)\n",
        "    embeddingWeights = np.zeros((vocab_size , embed.shape[1]))\n",
        "    n = 0\n",
        "    words_missed = []\n",
        "    for k, v in vocab_2.iteritems():\n",
        "        try:\n",
        "            embeddingWeights[v] = embed[vocab_1[k]]\n",
        "        except:\n",
        "            n += 1\n",
        "            words_missed.append(k)\n",
        "            pass\n",
        "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
        "    return embeddingWeights\n",
        "\n",
        "def transfer_learning_2(dict_1, dict_2, model_type, vector_type, embed_size):\n",
        "    trainX = dict_2[\"trainX\"]\n",
        "    trainY = dict_2[\"trainY\"]\n",
        "    testX = dict_2[\"testX\"]\n",
        "    testY = dict_2[\"testY\"]\n",
        "\n",
        "    EPOCHS = 5\n",
        "    BATCH_SIZE = 128\n",
        "    vocab_processor = dict_2[\"vocab\"]\n",
        "    vocab_size = len(vocab_processor.vocabulary_)\n",
        "    print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
        "\n",
        "    vocab = vocab_processor.vocabulary_._mapping\n",
        "\n",
        "    print(\"Running Model: \" + model_type + \" with word vector initiliazed with word vectors trained on dataset 1.\")\n",
        "    model = get_model(model_type, trainX.shape[1], vocab_size, embed_size, NUM_CLASSES, LEARN_RATE)\n",
        "\n",
        "    if(model_type == 'cnn'):\n",
        "        embeddingWeights = tflearn.get_layer_variables_by_name('EmbeddingLayer')[0]\n",
        "        embed = dict_1[\"model\"].get_weights(embeddingWeights)\n",
        "        model.set_weights(embeddingWeights, map_embedding_trained_weights(embed, dict_1[\"vocab\"].vocabulary_._mapping, vocab))\n",
        "        model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
        "    else:\n",
        "        embed = dict_1[\"model\"].layers[0].get_weights()[0]\n",
        "        model.layers[0].set_weights([map_embedding_trained_weights(embed, dict_1[\"vocab\"].vocabulary_._mapping, vocab)])\n",
        "        model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
        "              verbose=1)\n",
        "\n",
        "    evaluate_model(model, testX, testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPSpTySRQ5jV"
      },
      "outputs": [],
      "source": [
        "def transfer_learning_3(dict_1, dict_2, model_type, vector_type, embed_size):\n",
        " \n",
        "    trainX = dict_2[\"trainX\"]\n",
        "    trainY = dict_2[\"trainY\"]\n",
        "    testX = dict_2[\"testX\"]\n",
        "    testY = dict_2[\"testY\"]\n",
        "\n",
        "    vocab_processor = dict_2[\"vocab\"]\n",
        "    vocab_size = len(vocab_processor.vocabulary_)\n",
        "    print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
        "\n",
        "    vocab = vocab_processor.vocabulary_._mapping\n",
        "\n",
        "    print(\"Running Model: \" + model_type + \" with word vector initiliazed with word vectors trainned on dataset 1.\")\n",
        "    model = get_model(model_type, trainX.shape[1], vocab_size, embed_size, NUM_CLASSES, LEARN_RATE)\n",
        "\n",
        "    EPOCHS = 5\n",
        "    BATCH_SIZE = 128\n",
        "    \n",
        "    if(model_type == 'cnn'):\n",
        "\n",
        "        embeddingWeights = tflearn.get_layer_variables_by_name('EmbeddingLayer')[0]\n",
        "        embed = dict_1[\"model\"].get_weights(embeddingWeights)\n",
        "        model.set_weights(embeddingWeights, map_embedding_trained_weights(embed, dict_1[\"vocab\"].vocabulary_._mapping, vocab))\n",
        "\n",
        "        layer1Weights = tflearn.get_layer_variables_by_name('layer_1')[0]\n",
        "        model.set_weights(layer1Weights, dict_1[\"model\"].get_weights(layer1Weights))\n",
        "\n",
        "        layer2Weights = tflearn.get_layer_variables_by_name('layer_2')[0]\n",
        "        model.set_weights(layer2Weights, dict_1[\"model\"].get_weights(layer2Weights))\n",
        "\n",
        "        layer3Weights = tflearn.get_layer_variables_by_name('layer_1')[0]\n",
        "        model.set_weights(layer3Weights, dict_1[\"model\"].get_weights(layer3Weights))\n",
        "\n",
        "        fcWeights = tflearn.get_layer_variables_by_name('layer_1')[0]\n",
        "        model.set_weights(fcWeights, dict_1[\"model\"].get_weights(fcWeights))\n",
        "\n",
        "        model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
        "\n",
        "    elif(model_type == 'blstm_attention'):\n",
        "        embed = dict_1[\"model\"].layers[0].get_weights()[0]\n",
        "        model.layers[0].set_weights([map_embedding_trained_weights(embed, dict_1[\"vocab\"].vocabulary_._mapping, vocab)])\n",
        "        model.layers[2].set_weights(dict_1[\"model\"].layers[2].get_weights())\n",
        "        model.layers[3].set_weights(dict_1[\"model\"].layers[3].get_weights())\n",
        "        model.layers[5].set_weights(dict_1[\"model\"].layers[5].get_weights())\n",
        "        model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
        "              verbose=1)\n",
        "    else:\n",
        "        embed = dict_1[\"model\"].layers[0].get_weights()[0]\n",
        "        model.layers[0].set_weights([map_embedding_trained_weights(embed, dict_1[\"vocab\"].vocabulary_._mapping, vocab)])\n",
        "        model.layers[2].set_weights(dict_1[\"model\"].layers[2].get_weights())\n",
        "        model.layers[4].set_weights(dict_1[\"model\"].layers[4].get_weights())\n",
        "        model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
        "              verbose=1)\n",
        "\n",
        "    evaluate_model(model, testX, testY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98dBmfiBRTmw"
      },
      "outputs": [],
      "source": [
        "data_1 = \"formspring\"\n",
        "data_2 = \"twitter\"\n",
        "data_3 = \"wiki\"\n",
        "model_type =\"blstm\"\n",
        "vector_type = \"glove\"\n",
        "embed_size = 50\n",
        "oversampling_rate = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfsSvOoqTagt"
      },
      "outputs": [],
      "source": [
        "pip install -U tensorflow==2.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idSLwP9A2YZ2"
      },
      "outputs": [],
      "source": [
        "pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsQQJC4k2Eso"
      },
      "outputs": [],
      "source": [
        "pip install learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnVygaIk1gBS"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWaHdZ9KRTsm"
      },
      "outputs": [],
      "source": [
        "x_text, labels = get_data(data_1, oversampling_rate)\n",
        "dict_1 =  train(data_1, x_text, labels, model_type, vector_type, embed_size)\n",
        "\n",
        "x_text, labels = get_data(data_2, oversampling_rate)\n",
        "dict_2 =  train(data_2, x_text, labels, model_type, vector_type, embed_size)\n",
        "\n",
        "x_text, labels = get_data(data_3, oversampling_rate)\n",
        "dict_3 =  train(data_3, x_text, labels, model_type, vector_type, embed_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTWVUgz4RTug"
      },
      "outputs": [],
      "source": [
        "transfer_learning = {\n",
        "    1: transfer_learning_1,\n",
        "    2: transfer_learning_2,\n",
        "    3: transfer_learning_3\n",
        "}\n",
        "\n",
        "data_dict = [dict_1, dict_2, dict_3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LnZaTyhRTxF"
      },
      "outputs": [],
      "source": [
        "def get_results(data_dict, model_type, vector_type, embed_size, ind):\n",
        "    for i in range(len(data_dict)):\n",
        "        for j in range(i+1, len(data_dict)):\n",
        "            transfer_learning[ind](data_dict[i],data_dict[j], model_type, vector_type, embed_size) \n",
        "            transfer_learning[ind](data_dict[j],data_dict[i], model_type, vector_type, embed_size) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjNDUc2GRTzi"
      },
      "outputs": [],
      "source": [
        "print(dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcN8vGLgRT2D"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CYyber bullying paper.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}